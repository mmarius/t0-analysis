{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from utils import load_hidden_representations_from_hdf5, read_templates_from_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "log_dir = \"/logfiles\"\n",
    "model = \"bigscience-T0_3B\" # bigscience-T0_B or bigscience-T0\n",
    "module = \"encoder\" # encoder\n",
    "# task = \"rte\"\n",
    "task = \"cb\"\n",
    "# task = \"wic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert module == \"encoder\" # TODO(mm): support decoder as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>template</th>\n",
       "      <th>category</th>\n",
       "      <th>includes_labels</th>\n",
       "      <th>shuffle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>null_pattern</td>\n",
       "      <td>{premise} {hypothesis}</td>\n",
       "      <td>neutral</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>null_pattern_reversed</td>\n",
       "      <td>{hypothesis} {premise}</td>\n",
       "      <td>neutral</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt_3_true_false_neither</td>\n",
       "      <td>{premise} Question: {hypothesis} True, False, ...</td>\n",
       "      <td>instructive</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt_3_yes_no_maybe</td>\n",
       "      <td>{premise} Question: {hypothesis} Yes, No, or M...</td>\n",
       "      <td>instructive</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mnli_crowdsource</td>\n",
       "      <td>{premise} Using only the above description and...</td>\n",
       "      <td>instructive</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>always_sometimes_never</td>\n",
       "      <td>Suppose it's true that {premise} Then, is \"{hy...</td>\n",
       "      <td>instructive</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>based_on_previous_passage</td>\n",
       "      <td>{premise} Based on the previous passage, is it...</td>\n",
       "      <td>instructive</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>infer</td>\n",
       "      <td>Suppose {premise} Can we infer that \"{hypothes...</td>\n",
       "      <td>instructive</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>claim</td>\n",
       "      <td>{premise} Based on that information, is the cl...</td>\n",
       "      <td>instructive</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>consider</td>\n",
       "      <td>{premise} Keeping in mind the above text, cons...</td>\n",
       "      <td>instructive</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>follow</td>\n",
       "      <td>Given that {premise} Does it follow that {hypo...</td>\n",
       "      <td>instructive</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>imply</td>\n",
       "      <td>{premise} Question: Does this imply that \"{hyp...</td>\n",
       "      <td>instructive</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>guaranteed</td>\n",
       "      <td>Given {premise} Is it guaranteed true that \"{h...</td>\n",
       "      <td>instructive</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>guaranteed_possible</td>\n",
       "      <td>Assume it is true that {premise} Therefor, \"{h...</td>\n",
       "      <td>instructive</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>justified</td>\n",
       "      <td>{premise} Are we justified in saying that \"{hy...</td>\n",
       "      <td>instructive</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>must_be_true</td>\n",
       "      <td>Given that {premise} Therefore, it must be tru...</td>\n",
       "      <td>instructive</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>should_assume</td>\n",
       "      <td>Given {premise} Should we assume that \"{hypoth...</td>\n",
       "      <td>instructive</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>take_the_following</td>\n",
       "      <td>Take the following as truth: {premise} Then th...</td>\n",
       "      <td>instructive</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         name  \\\n",
       "0                null_pattern   \n",
       "1       null_pattern_reversed   \n",
       "2    gpt_3_true_false_neither   \n",
       "3          gpt_3_yes_no_maybe   \n",
       "4            mnli_crowdsource   \n",
       "5      always_sometimes_never   \n",
       "6   based_on_previous_passage   \n",
       "7                       infer   \n",
       "8                       claim   \n",
       "9                    consider   \n",
       "10                     follow   \n",
       "11                      imply   \n",
       "12                 guaranteed   \n",
       "13        guaranteed_possible   \n",
       "14                  justified   \n",
       "15               must_be_true   \n",
       "16              should_assume   \n",
       "17         take_the_following   \n",
       "\n",
       "                                             template     category  \\\n",
       "0                              {premise} {hypothesis}      neutral   \n",
       "1                              {hypothesis} {premise}      neutral   \n",
       "2   {premise} Question: {hypothesis} True, False, ...  instructive   \n",
       "3   {premise} Question: {hypothesis} Yes, No, or M...  instructive   \n",
       "4   {premise} Using only the above description and...  instructive   \n",
       "5   Suppose it's true that {premise} Then, is \"{hy...  instructive   \n",
       "6   {premise} Based on the previous passage, is it...  instructive   \n",
       "7   Suppose {premise} Can we infer that \"{hypothes...  instructive   \n",
       "8   {premise} Based on that information, is the cl...  instructive   \n",
       "9   {premise} Keeping in mind the above text, cons...  instructive   \n",
       "10  Given that {premise} Does it follow that {hypo...  instructive   \n",
       "11  {premise} Question: Does this imply that \"{hyp...  instructive   \n",
       "12  Given {premise} Is it guaranteed true that \"{h...  instructive   \n",
       "13  Assume it is true that {premise} Therefor, \"{h...  instructive   \n",
       "14  {premise} Are we justified in saying that \"{hy...  instructive   \n",
       "15  Given that {premise} Therefore, it must be tru...  instructive   \n",
       "16  Given {premise} Should we assume that \"{hypoth...  instructive   \n",
       "17  Take the following as truth: {premise} Then th...  instructive   \n",
       "\n",
       "    includes_labels  shuffle  \n",
       "0             False    False  \n",
       "1             False    False  \n",
       "2              True    False  \n",
       "3              True    False  \n",
       "4              True    False  \n",
       "5              True    False  \n",
       "6              True    False  \n",
       "7              True    False  \n",
       "8              True    False  \n",
       "9              True    False  \n",
       "10             True    False  \n",
       "11             True    False  \n",
       "12             True    False  \n",
       "13             True    False  \n",
       "14             True    False  \n",
       "15             True    False  \n",
       "16             True    False  \n",
       "17             True    False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = read_templates_from_file(f\"/t0-analysis/prompts/{task}.csv\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unison_shuffled_copies(a, b):\n",
    "    # from: https://stackoverflow.com/questions/4601373/better-way-to-shuffle-two-numpy-arrays-in-unison\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RTE patterns\n",
    "# use_pattern = [\n",
    "#     \"null_pattern\",\n",
    "#     \"null_pattern_reversed\",\n",
    "#     \"gpt_3_yes_no\",\n",
    "#     \"gpt_3_yes_no_shuffled\",\n",
    "#     \"gpt_3_true_false\",\n",
    "#     \"gpt_3_true_false_shuffled\",\n",
    "#     \"start_with_the\",\n",
    "#     \"mnli_crowdsource\",\n",
    "#     \"based_on_previous_passage\",\n",
    "#     \"infer\",\n",
    "#     \"follow\",\n",
    "#     \"imply\",\n",
    "#     \"guaranteed\",\n",
    "#     \"justified\", \n",
    "#     \"must_be_true\",\n",
    "#     \"should_assume\"\n",
    "# ]\n",
    "\n",
    "# CB patterns\n",
    "use_pattern = [\n",
    "    \"null_pattern\",\n",
    "    \"null_pattern_reversed\",\n",
    "    \"gpt_3_true_false_neither\",\n",
    "    \"gpt_3_yes_no_maybe\",\n",
    "    \"mnli_crowdsource\",\n",
    "    \"always_sometimes_never\",\n",
    "    \"based_on_previous_passage\",\n",
    "    \"infer\",\n",
    "    \"claim\",\n",
    "    \"consider\",\n",
    "    \"follow\",\n",
    "    \"imply\",\n",
    "    \"guaranteed\",\n",
    "    \"guaranteed_possible\",\n",
    "    \"justified\",\n",
    "    \"must_be_true\",\n",
    "    \"should_assume\",\n",
    "    \"take_the_following\",\n",
    "]\n",
    "\n",
    "# WIC patterns\n",
    "# use_pattern = [\n",
    "#     \"gpt_3\",\n",
    "#     \"gpt_3_yes_no\",\n",
    "#     \"affirmation\",\n",
    "#     \"grammar_homework\",\n",
    "#     \"polysemous\",\n",
    "#     \"question_context\",\n",
    "#     \"question_meaning\",\n",
    "#     \"question_meaning_yes_no\",\n",
    "#     \"same_sense\",\n",
    "#     \"similar_sense\",\n",
    "#     \"similar_sense_yes_no\",\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer= 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5182.95it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5369.81it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5051.31it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 4731.59it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5082.57it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5038.31it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5190.17it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5227.36it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5227.94it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5302.29it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5155.31it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5323.20it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5077.19it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 4847.11it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 4983.05it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 4740.76it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5184.21it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5161.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1008, 2048) (1008,)\n",
      "PCA for prompts: ['null_pattern', 'null_pattern_reversed', 'gpt_3_true_false_neither', 'gpt_3_yes_no_maybe', 'mnli_crowdsource', 'always_sometimes_never', 'based_on_previous_passage', 'infer', 'claim', 'consider', 'follow', 'imply', 'guaranteed', 'guaranteed_possible', 'justified', 'must_be_true', 'should_assume', 'take_the_following']\n",
      "model:bigscience-T0_3B; module:encoder; layer:0; n_components: 1; variance explained: [0.9334017]\n",
      "0.9334017\n",
      "model:bigscience-T0_3B; module:encoder; layer:0; n_components: 2; variance explained: [0.93340164 0.01033186]\n",
      "0.9437335\n",
      "model:bigscience-T0_3B; module:encoder; layer:0; n_components: 3; variance explained: [0.93340164 0.01033187 0.00871112]\n",
      "0.9524446\n",
      "model:bigscience-T0_3B; module:encoder; layer:0; n_components: 4; variance explained: [0.93340164 0.01033188 0.00871113 0.00424156]\n",
      "0.9566862\n",
      "\n",
      "\n",
      "layer= 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 1923.57it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 1811.58it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5107.55it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5016.90it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5189.71it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5022.37it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 4946.74it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5207.08it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5203.51it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5211.93it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5193.15it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5269.34it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5214.13it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5260.61it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5245.69it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5319.10it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5067.77it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5182.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1008, 2048) (1008,)\n",
      "PCA for prompts: ['null_pattern', 'null_pattern_reversed', 'gpt_3_true_false_neither', 'gpt_3_yes_no_maybe', 'mnli_crowdsource', 'always_sometimes_never', 'based_on_previous_passage', 'infer', 'claim', 'consider', 'follow', 'imply', 'guaranteed', 'guaranteed_possible', 'justified', 'must_be_true', 'should_assume', 'take_the_following']\n",
      "model:bigscience-T0_3B; module:encoder; layer:1; n_components: 1; variance explained: [0.26860443]\n",
      "0.26860443\n",
      "model:bigscience-T0_3B; module:encoder; layer:1; n_components: 2; variance explained: [0.2686043  0.20742142]\n",
      "0.47602573\n",
      "model:bigscience-T0_3B; module:encoder; layer:1; n_components: 3; variance explained: [0.26860407 0.20742129 0.09264082]\n",
      "0.56866616\n",
      "model:bigscience-T0_3B; module:encoder; layer:1; n_components: 4; variance explained: [0.26860434 0.20742121 0.09264082 0.05665776]\n",
      "0.62532413\n",
      "\n",
      "\n",
      "layer= 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 2063.71it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 2094.07it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 4622.46it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5290.35it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5135.02it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5319.10it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5425.26it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5306.97it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5309.01it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5265.80it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5365.03it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5369.08it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5271.95it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5314.29it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5213.32it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5279.17it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5307.69it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5229.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1008, 2048) (1008,)\n",
      "PCA for prompts: ['null_pattern', 'null_pattern_reversed', 'gpt_3_true_false_neither', 'gpt_3_yes_no_maybe', 'mnli_crowdsource', 'always_sometimes_never', 'based_on_previous_passage', 'infer', 'claim', 'consider', 'follow', 'imply', 'guaranteed', 'guaranteed_possible', 'justified', 'must_be_true', 'should_assume', 'take_the_following']\n",
      "model:bigscience-T0_3B; module:encoder; layer:2; n_components: 1; variance explained: [0.98610854]\n",
      "0.98610854\n",
      "model:bigscience-T0_3B; module:encoder; layer:2; n_components: 2; variance explained: [0.98610866 0.01023586]\n",
      "0.9963445\n",
      "model:bigscience-T0_3B; module:encoder; layer:2; n_components: 3; variance explained: [9.8610854e-01 1.0235861e-02 7.4431644e-04]\n",
      "0.99708873\n",
      "model:bigscience-T0_3B; module:encoder; layer:2; n_components: 4; variance explained: [9.8610854e-01 1.0235857e-02 7.4431667e-04 5.1048130e-04]\n",
      "0.9975992\n",
      "\n",
      "\n",
      "layer= 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 1834.65it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 1848.58it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 4906.03it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5228.29it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5365.28it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5223.99it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5286.54it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5318.26it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5318.02it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5309.13it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5296.79it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5233.65it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5251.55it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5283.09it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5272.42it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5188.68it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5243.23it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5186.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1008, 2048) (1008,)\n",
      "PCA for prompts: ['null_pattern', 'null_pattern_reversed', 'gpt_3_true_false_neither', 'gpt_3_yes_no_maybe', 'mnli_crowdsource', 'always_sometimes_never', 'based_on_previous_passage', 'infer', 'claim', 'consider', 'follow', 'imply', 'guaranteed', 'guaranteed_possible', 'justified', 'must_be_true', 'should_assume', 'take_the_following']\n",
      "model:bigscience-T0_3B; module:encoder; layer:3; n_components: 1; variance explained: [0.98944396]\n",
      "0.98944396\n",
      "model:bigscience-T0_3B; module:encoder; layer:3; n_components: 2; variance explained: [0.9894441 0.0071629]\n",
      "0.99660695\n",
      "model:bigscience-T0_3B; module:encoder; layer:3; n_components: 3; variance explained: [9.894441e-01 7.162894e-03 7.858323e-04]\n",
      "0.9973928\n",
      "model:bigscience-T0_3B; module:encoder; layer:3; n_components: 4; variance explained: [9.8944396e-01 7.1628978e-03 7.8583311e-04 6.2971300e-04]\n",
      "0.9980224\n",
      "\n",
      "\n",
      "layer= 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 1348.97it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 2297.21it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5396.71it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5177.01it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 4872.85it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5193.61it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5169.61it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5164.94it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 4788.90it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5015.29it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 4640.82it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5068.86it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 4967.24it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5101.34it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5012.83it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 4975.24it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 4909.21it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 4877.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1008, 2048) (1008,)\n",
      "PCA for prompts: ['null_pattern', 'null_pattern_reversed', 'gpt_3_true_false_neither', 'gpt_3_yes_no_maybe', 'mnli_crowdsource', 'always_sometimes_never', 'based_on_previous_passage', 'infer', 'claim', 'consider', 'follow', 'imply', 'guaranteed', 'guaranteed_possible', 'justified', 'must_be_true', 'should_assume', 'take_the_following']\n",
      "model:bigscience-T0_3B; module:encoder; layer:4; n_components: 1; variance explained: [0.9903445]\n",
      "0.9903445\n",
      "model:bigscience-T0_3B; module:encoder; layer:4; n_components: 2; variance explained: [0.9903445  0.00600426]\n",
      "0.9963488\n",
      "model:bigscience-T0_3B; module:encoder; layer:4; n_components: 3; variance explained: [9.9034452e-01 6.0042636e-03 7.6484430e-04]\n",
      "0.99711365\n",
      "model:bigscience-T0_3B; module:encoder; layer:4; n_components: 4; variance explained: [9.9034452e-01 6.0042590e-03 7.6484529e-04 7.4030156e-04]\n",
      "0.99785393\n",
      "\n",
      "\n",
      "layer= 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 1237.36it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 3256.45it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5194.41it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 4743.54it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5291.78it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5182.84it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5289.76it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5072.37it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5140.08it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5110.66it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5204.77it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5197.98it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5099.90it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5154.86it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5153.16it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5185.47it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5233.77it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5299.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1008, 2048) (1008,)\n",
      "PCA for prompts: ['null_pattern', 'null_pattern_reversed', 'gpt_3_true_false_neither', 'gpt_3_yes_no_maybe', 'mnli_crowdsource', 'always_sometimes_never', 'based_on_previous_passage', 'infer', 'claim', 'consider', 'follow', 'imply', 'guaranteed', 'guaranteed_possible', 'justified', 'must_be_true', 'should_assume', 'take_the_following']\n",
      "model:bigscience-T0_3B; module:encoder; layer:5; n_components: 1; variance explained: [0.9908122]\n",
      "0.9908122\n",
      "model:bigscience-T0_3B; module:encoder; layer:5; n_components: 2; variance explained: [0.9908122  0.00527297]\n",
      "0.99608517\n",
      "model:bigscience-T0_3B; module:encoder; layer:5; n_components: 3; variance explained: [9.9081200e-01 5.2729738e-03 8.8942464e-04]\n",
      "0.9969744\n",
      "model:bigscience-T0_3B; module:encoder; layer:5; n_components: 4; variance explained: [9.9081218e-01 5.2729757e-03 8.8942290e-04 6.8926078e-04]\n",
      "0.99766386\n",
      "\n",
      "\n",
      "layer= 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 1856.02it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 1783.25it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 4838.12it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 4742.48it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5084.56it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5137.16it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 4796.72it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5169.04it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5203.85it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5215.29it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5204.77it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5226.66it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5236.92it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5173.02it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5115.78it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5133.56it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5151.13it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5179.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1008, 2048) (1008,)\n",
      "PCA for prompts: ['null_pattern', 'null_pattern_reversed', 'gpt_3_true_false_neither', 'gpt_3_yes_no_maybe', 'mnli_crowdsource', 'always_sometimes_never', 'based_on_previous_passage', 'infer', 'claim', 'consider', 'follow', 'imply', 'guaranteed', 'guaranteed_possible', 'justified', 'must_be_true', 'should_assume', 'take_the_following']\n",
      "model:bigscience-T0_3B; module:encoder; layer:6; n_components: 1; variance explained: [0.9995892]\n",
      "0.9995892\n",
      "model:bigscience-T0_3B; module:encoder; layer:6; n_components: 2; variance explained: [9.9958920e-01 2.7347245e-04]\n",
      "0.9998627\n",
      "model:bigscience-T0_3B; module:encoder; layer:6; n_components: 3; variance explained: [9.9958920e-01 2.7347263e-04 3.0252961e-05]\n",
      "0.99989295\n",
      "model:bigscience-T0_3B; module:encoder; layer:6; n_components: 4; variance explained: [9.9958920e-01 2.7347245e-04 3.0252924e-05 2.1571437e-05]\n",
      "0.9999145\n",
      "\n",
      "\n",
      "layer= 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 4007.32it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 2349.14it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 2008.84it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5161.20it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5155.20it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5194.41it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5405.28it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5234.12it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5182.27it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5225.04it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5265.56it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5230.74it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5287.85it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5109.44it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5289.52it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5254.96it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5189.94it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5239.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1008, 2048) (1008,)\n",
      "PCA for prompts: ['null_pattern', 'null_pattern_reversed', 'gpt_3_true_false_neither', 'gpt_3_yes_no_maybe', 'mnli_crowdsource', 'always_sometimes_never', 'based_on_previous_passage', 'infer', 'claim', 'consider', 'follow', 'imply', 'guaranteed', 'guaranteed_possible', 'justified', 'must_be_true', 'should_assume', 'take_the_following']\n",
      "model:bigscience-T0_3B; module:encoder; layer:7; n_components: 1; variance explained: [0.99956095]\n",
      "0.99956095\n",
      "model:bigscience-T0_3B; module:encoder; layer:7; n_components: 2; variance explained: [9.9956095e-01 2.7372042e-04]\n",
      "0.99983466\n",
      "model:bigscience-T0_3B; module:encoder; layer:7; n_components: 3; variance explained: [9.9956095e-01 2.7372033e-04 3.6880487e-05]\n",
      "0.99987155\n",
      "model:bigscience-T0_3B; module:encoder; layer:7; n_components: 4; variance explained: [9.9956095e-01 2.7372033e-04 3.6880410e-05 2.4751727e-05]\n",
      "0.9998963\n",
      "\n",
      "\n",
      "layer= 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 2040.85it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 2070.60it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 4604.43it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5313.93it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5113.67it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5070.72it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5342.45it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5200.86it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 4968.92it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5025.27it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5291.07it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5303.85it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5213.90it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5259.20it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5192.58it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5198.32it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 4733.79it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 4651.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1008, 2048) (1008,)\n",
      "PCA for prompts: ['null_pattern', 'null_pattern_reversed', 'gpt_3_true_false_neither', 'gpt_3_yes_no_maybe', 'mnli_crowdsource', 'always_sometimes_never', 'based_on_previous_passage', 'infer', 'claim', 'consider', 'follow', 'imply', 'guaranteed', 'guaranteed_possible', 'justified', 'must_be_true', 'should_assume', 'take_the_following']\n",
      "model:bigscience-T0_3B; module:encoder; layer:8; n_components: 1; variance explained: [0.9995339]\n",
      "0.9995339\n",
      "model:bigscience-T0_3B; module:encoder; layer:8; n_components: 2; variance explained: [9.9953389e-01 2.5841262e-04]\n",
      "0.9997923\n",
      "model:bigscience-T0_3B; module:encoder; layer:8; n_components: 3; variance explained: [9.9953389e-01 2.5841253e-04 5.1561612e-05]\n",
      "0.99984384\n",
      "model:bigscience-T0_3B; module:encoder; layer:8; n_components: 4; variance explained: [9.9953389e-01 2.5841244e-04 5.1561583e-05 2.8426562e-05]\n",
      "0.99987227\n",
      "\n",
      "\n",
      "layer= 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 2091.94it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 3676.91it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 3109.57it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5225.38it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5313.69it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5373.50it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5370.06it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5301.34it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5210.78it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5292.85it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 4919.49it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 4215.76it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 4143.76it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 4092.22it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 4494.90it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5236.57it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5063.51it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 5180.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1008, 2048) (1008,)\n",
      "PCA for prompts: ['null_pattern', 'null_pattern_reversed', 'gpt_3_true_false_neither', 'gpt_3_yes_no_maybe', 'mnli_crowdsource', 'always_sometimes_never', 'based_on_previous_passage', 'infer', 'claim', 'consider', 'follow', 'imply', 'guaranteed', 'guaranteed_possible', 'justified', 'must_be_true', 'should_assume', 'take_the_following']\n",
      "model:bigscience-T0_3B; module:encoder; layer:9; n_components: 1; variance explained: [0.99948126]\n",
      "0.99948126\n",
      "model:bigscience-T0_3B; module:encoder; layer:9; n_components: 2; variance explained: [9.9948126e-01 2.5689061e-04]\n",
      "0.99973816\n",
      "model:bigscience-T0_3B; module:encoder; layer:9; n_components: 3; variance explained: [9.9948114e-01 2.5689069e-04 6.2855164e-05]\n",
      "0.9998009\n",
      "model:bigscience-T0_3B; module:encoder; layer:9; n_components: 4; variance explained: [9.9948114e-01 2.5689078e-04 6.2855150e-05 3.3748031e-05]\n",
      "0.99983466\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for layer in range(0, 10):\n",
    "# for layer in range(0, 25):\n",
    "# for layer in range(24, 25):\n",
    "    print('layer=', layer)\n",
    "    file_names, prompt_names = [], []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        if row['name'] in use_pattern:\n",
    "            file_names.append(f\"{task}/{model}/{module}/{row['name']}/hidden_represenations_layer{layer}_avg.hdf5\",)\n",
    "            prompt_names.append(row['name'])\n",
    "\n",
    "\n",
    "    # load hidden representations from hdf5 file\n",
    "    representations = None\n",
    "    classes = []\n",
    "    n_sequences = 0\n",
    "\n",
    "    for idx, file_name in enumerate(file_names):\n",
    "        hidden_representations = load_hidden_representations_from_hdf5(os.path.join(log_dir, file_name))\n",
    "        # print(hidden_representations.shape)\n",
    "        n_sequences = hidden_representations.shape[0]\n",
    "\n",
    "        if representations is None:\n",
    "            representations = hidden_representations\n",
    "        else:\n",
    "            representations = np.concatenate((representations, hidden_representations), axis=0)\n",
    "\n",
    "        classes += n_sequences * [idx] # assign representations to classes\n",
    "    \n",
    "    classes = np.asarray(classes)\n",
    "\n",
    "    # shuffle representations and classes\n",
    "    X, y = unison_shuffled_copies(representations, classes)\n",
    "    print(X.shape, y.shape)\n",
    "\n",
    "    # perform PCA on hidden representations\n",
    "    print('PCA for prompts:', prompt_names)\n",
    "\n",
    "    for n_components in range(1, 5):\n",
    "        pca = PCA(n_components=n_components)\n",
    "        pca.fit(X)\n",
    "\n",
    "        # variance explained by each of the principal components\n",
    "        print(f\"model:{model}; module:{module}; layer:{layer}; n_components: {n_components}; variance explained: {pca.explained_variance_ratio_}\")\n",
    "        print(np.sum(pca.explained_variance_ratio_))\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
