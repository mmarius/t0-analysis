{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from utils import load_hidden_representations_from_hdf5, read_templates_from_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "log_dir = \"/logfiles\"\n",
    "model = \"bigscience-T0_3B\" # bigscience-T0_B or bigscience-T0\n",
    "module = \"encoder\" # encoder\n",
    "# task = \"rte\"\n",
    "task = \"cb\"\n",
    "# task = \"wic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert module == \"encoder\" # TODO(mm): support decoder as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>template</th>\n",
       "      <th>category</th>\n",
       "      <th>includes_labels</th>\n",
       "      <th>shuffle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>null_pattern</td>\n",
       "      <td>{premise} {hypothesis}</td>\n",
       "      <td>neutral</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>null_pattern_reversed</td>\n",
       "      <td>{hypothesis} {premise}</td>\n",
       "      <td>neutral</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt_3_true_false_neither</td>\n",
       "      <td>{premise} Question: {hypothesis} True, False, ...</td>\n",
       "      <td>instructive</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt_3_yes_no_maybe</td>\n",
       "      <td>{premise} Question: {hypothesis} Yes, No, or M...</td>\n",
       "      <td>instructive</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mnli_crowdsource</td>\n",
       "      <td>{premise} Using only the above description and...</td>\n",
       "      <td>instructive</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>always_sometimes_never</td>\n",
       "      <td>Suppose it's true that {premise} Then, is \"{hy...</td>\n",
       "      <td>instructive</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>based_on_previous_passage</td>\n",
       "      <td>{premise} Based on the previous passage, is it...</td>\n",
       "      <td>instructive</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>infer</td>\n",
       "      <td>Suppose {premise} Can we infer that \"{hypothes...</td>\n",
       "      <td>instructive</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>claim</td>\n",
       "      <td>{premise} Based on that information, is the cl...</td>\n",
       "      <td>instructive</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>consider</td>\n",
       "      <td>{premise} Keeping in mind the above text, cons...</td>\n",
       "      <td>instructive</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>follow</td>\n",
       "      <td>Given that {premise} Does it follow that {hypo...</td>\n",
       "      <td>instructive</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>imply</td>\n",
       "      <td>{premise} Question: Does this imply that \"{hyp...</td>\n",
       "      <td>instructive</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>guaranteed</td>\n",
       "      <td>Given {premise} Is it guaranteed true that \"{h...</td>\n",
       "      <td>instructive</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>guaranteed_possible</td>\n",
       "      <td>Assume it is true that {premise} Therefor, \"{h...</td>\n",
       "      <td>instructive</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>justified</td>\n",
       "      <td>{premise} Are we justified in saying that \"{hy...</td>\n",
       "      <td>instructive</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>must_be_true</td>\n",
       "      <td>Given that {premise} Therefore, it must be tru...</td>\n",
       "      <td>instructive</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>should_assume</td>\n",
       "      <td>Given {premise} Should we assume that \"{hypoth...</td>\n",
       "      <td>instructive</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>take_the_following</td>\n",
       "      <td>Take the following as truth: {premise} Then th...</td>\n",
       "      <td>instructive</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         name  \\\n",
       "0                null_pattern   \n",
       "1       null_pattern_reversed   \n",
       "2    gpt_3_true_false_neither   \n",
       "3          gpt_3_yes_no_maybe   \n",
       "4            mnli_crowdsource   \n",
       "5      always_sometimes_never   \n",
       "6   based_on_previous_passage   \n",
       "7                       infer   \n",
       "8                       claim   \n",
       "9                    consider   \n",
       "10                     follow   \n",
       "11                      imply   \n",
       "12                 guaranteed   \n",
       "13        guaranteed_possible   \n",
       "14                  justified   \n",
       "15               must_be_true   \n",
       "16              should_assume   \n",
       "17         take_the_following   \n",
       "\n",
       "                                             template     category  \\\n",
       "0                              {premise} {hypothesis}      neutral   \n",
       "1                              {hypothesis} {premise}      neutral   \n",
       "2   {premise} Question: {hypothesis} True, False, ...  instructive   \n",
       "3   {premise} Question: {hypothesis} Yes, No, or M...  instructive   \n",
       "4   {premise} Using only the above description and...  instructive   \n",
       "5   Suppose it's true that {premise} Then, is \"{hy...  instructive   \n",
       "6   {premise} Based on the previous passage, is it...  instructive   \n",
       "7   Suppose {premise} Can we infer that \"{hypothes...  instructive   \n",
       "8   {premise} Based on that information, is the cl...  instructive   \n",
       "9   {premise} Keeping in mind the above text, cons...  instructive   \n",
       "10  Given that {premise} Does it follow that {hypo...  instructive   \n",
       "11  {premise} Question: Does this imply that \"{hyp...  instructive   \n",
       "12  Given {premise} Is it guaranteed true that \"{h...  instructive   \n",
       "13  Assume it is true that {premise} Therefor, \"{h...  instructive   \n",
       "14  {premise} Are we justified in saying that \"{hy...  instructive   \n",
       "15  Given that {premise} Therefore, it must be tru...  instructive   \n",
       "16  Given {premise} Should we assume that \"{hypoth...  instructive   \n",
       "17  Take the following as truth: {premise} Then th...  instructive   \n",
       "\n",
       "    includes_labels  shuffle  \n",
       "0             False    False  \n",
       "1             False    False  \n",
       "2              True    False  \n",
       "3              True    False  \n",
       "4              True    False  \n",
       "5              True    False  \n",
       "6              True    False  \n",
       "7              True    False  \n",
       "8              True    False  \n",
       "9              True    False  \n",
       "10             True    False  \n",
       "11             True    False  \n",
       "12             True    False  \n",
       "13             True    False  \n",
       "14             True    False  \n",
       "15             True    False  \n",
       "16             True    False  \n",
       "17             True    False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = read_templates_from_file(f\"/t0-analysis/prompts/{task}.csv\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train linear classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unison_shuffled_copies(a, b):\n",
    "    # from: https://stackoverflow.com/questions/4601373/better-way-to-shuffle-two-numpy-arrays-in-unison\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RTE patterns\n",
    "# use_pattern = [\n",
    "#     \"null_pattern\",\n",
    "#     \"null_pattern_reversed\",\n",
    "#     \"gpt_3_yes_no\",\n",
    "#     \"gpt_3_yes_no_shuffled\",\n",
    "#     \"gpt_3_true_false\",\n",
    "#     \"gpt_3_true_false_shuffled\",\n",
    "#     \"start_with_the\",\n",
    "#     \"mnli_crowdsource\",\n",
    "#     \"based_on_previous_passage\",\n",
    "#     \"infer\",\n",
    "#     \"follow\",\n",
    "#     \"imply\",\n",
    "#     \"guaranteed\",\n",
    "#     \"justified\", \n",
    "#     \"must_be_true\",\n",
    "#     \"should_assume\"\n",
    "# ]\n",
    "\n",
    "# CB patterns\n",
    "use_pattern = [\n",
    "    # \"null_pattern\",\n",
    "    # \"null_pattern_reversed\",\n",
    "    \"gpt_3_true_false_neither\",\n",
    "    \"gpt_3_yes_no_maybe\",\n",
    "    # \"mnli_crowdsource\",\n",
    "    # \"always_sometimes_never\",\n",
    "    # \"based_on_previous_passage\",\n",
    "    # \"infer\",\n",
    "    # \"claim\",\n",
    "    # \"consider\",\n",
    "    # \"follow\",\n",
    "    # \"imply\",\n",
    "    # \"guaranteed\",\n",
    "    # \"guaranteed_possible\",\n",
    "    # \"justified\",\n",
    "    # \"must_be_true\",\n",
    "    # \"should_assume\",\n",
    "    # \"take_the_following\",\n",
    "]\n",
    "\n",
    "# WIC patterns\n",
    "# use_pattern = [\n",
    "#     \"gpt_3\",\n",
    "#     \"gpt_3_yes_no\",\n",
    "#     \"affirmation\",\n",
    "#     \"grammar_homework\",\n",
    "#     \"polysemous\",\n",
    "#     \"question_context\",\n",
    "#     \"question_meaning\",\n",
    "#     \"question_meaning_yes_no\",\n",
    "#     \"same_sense\",\n",
    "#     \"similar_sense\",\n",
    "#     \"similar_sense_yes_no\",\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 3450.48it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 4952.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 2048) (112,)\n",
      "classifying between: ['gpt_3_true_false_neither', 'gpt_3_yes_no_maybe']\n",
      "layer=0; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "layer: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 4186.45it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 4269.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 2048) (112,)\n",
      "classifying between: ['gpt_3_true_false_neither', 'gpt_3_yes_no_maybe']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer=1; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "layer: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 4197.68it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 3901.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 2048) (112,)\n",
      "classifying between: ['gpt_3_true_false_neither', 'gpt_3_yes_no_maybe']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer=2; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "layer: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 3852.28it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 3959.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 2048) (112,)\n",
      "classifying between: ['gpt_3_true_false_neither', 'gpt_3_yes_no_maybe']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer=3; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "layer: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 3885.29it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 3672.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 2048) (112,)\n",
      "classifying between: ['gpt_3_true_false_neither', 'gpt_3_yes_no_maybe']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer=4; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "layer: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 3865.85it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 3958.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 2048) (112,)\n",
      "classifying between: ['gpt_3_true_false_neither', 'gpt_3_yes_no_maybe']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer=5; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "layer: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 3873.05it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 3996.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 2048) (112,)\n",
      "classifying between: ['gpt_3_true_false_neither', 'gpt_3_yes_no_maybe']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer=6; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "layer: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 3700.14it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 3946.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 2048) (112,)\n",
      "classifying between: ['gpt_3_true_false_neither', 'gpt_3_yes_no_maybe']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer=7; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "layer: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 3722.24it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 3716.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 2048) (112,)\n",
      "classifying between: ['gpt_3_true_false_neither', 'gpt_3_yes_no_maybe']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer=8; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "layer: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 3840.25it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 3858.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 2048) (112,)\n",
      "classifying between: ['gpt_3_true_false_neither', 'gpt_3_yes_no_maybe']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer=9; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "layer: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 3857.59it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 3820.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 2048) (112,)\n",
      "classifying between: ['gpt_3_true_false_neither', 'gpt_3_yes_no_maybe']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer=10; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "layer: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 3530.35it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 3759.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 2048) (112,)\n",
      "classifying between: ['gpt_3_true_false_neither', 'gpt_3_yes_no_maybe']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer=11; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "layer: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 3748.80it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 3839.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 2048) (112,)\n",
      "classifying between: ['gpt_3_true_false_neither', 'gpt_3_yes_no_maybe']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer=12; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "layer: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 2062.77it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 2961.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 2048) (112,)\n",
      "classifying between: ['gpt_3_true_false_neither', 'gpt_3_yes_no_maybe']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer=13; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "layer: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 3805.59it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 3743.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 2048) (112,)\n",
      "classifying between: ['gpt_3_true_false_neither', 'gpt_3_yes_no_maybe']\n",
      "layer=14; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "layer: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 3850.51it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 3736.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 2048) (112,)\n",
      "classifying between: ['gpt_3_true_false_neither', 'gpt_3_yes_no_maybe']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer=15; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "layer: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 3594.53it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 3695.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 2048) (112,)\n",
      "classifying between: ['gpt_3_true_false_neither', 'gpt_3_yes_no_maybe']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer=16; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "layer: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 3765.81it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 3754.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 2048) (112,)\n",
      "classifying between: ['gpt_3_true_false_neither', 'gpt_3_yes_no_maybe']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer=17; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "layer: 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 3506.00it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 3536.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 2048) (112,)\n",
      "classifying between: ['gpt_3_true_false_neither', 'gpt_3_yes_no_maybe']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer=18; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "layer: 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 3553.74it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 3479.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 2048) (112,)\n",
      "classifying between: ['gpt_3_true_false_neither', 'gpt_3_yes_no_maybe']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer=19; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "layer: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 3430.62it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 3391.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 2048) (112,)\n",
      "classifying between: ['gpt_3_true_false_neither', 'gpt_3_yes_no_maybe']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer=20; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "layer: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 3507.20it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 3666.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 2048) (112,)\n",
      "classifying between: ['gpt_3_true_false_neither', 'gpt_3_yes_no_maybe']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer=21; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "layer: 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 3512.98it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 3599.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 2048) (112,)\n",
      "classifying between: ['gpt_3_true_false_neither', 'gpt_3_yes_no_maybe']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer=22; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "layer: 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 3483.12it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 3542.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 2048) (112,)\n",
      "classifying between: ['gpt_3_true_false_neither', 'gpt_3_yes_no_maybe']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer=23; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "layer: 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 3515.81it/s]\n",
      "Reading embeddings: 100%|██████████| 56/56 [00:00<00:00, 3413.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 2048) (112,)\n",
      "classifying between: ['gpt_3_true_false_neither', 'gpt_3_yes_no_maybe']\n",
      "layer=24; accuracy on training data:  0.9821428571428571\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# for layer in range(0, 10):\n",
    "for layer in range(0, 25):\n",
    "# for layer in range(24, 25):\n",
    "    \n",
    "    print(\"layer:\", layer)\n",
    "    file_names, prompt_names = [], []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        if row['name'] in use_pattern:\n",
    "            file_names.append(f\"{task}/{model}/{module}/{row['name']}/hidden_represenations_layer{layer}_avg.hdf5\",)\n",
    "            prompt_names.append(row['name'])\n",
    "\n",
    "    # load hidden representations from hdf5 file\n",
    "    representations = None\n",
    "    classes = []\n",
    "    n_sequences = 0\n",
    "\n",
    "    for idx, file_name in enumerate(file_names):\n",
    "        hidden_representations = load_hidden_representations_from_hdf5(os.path.join(log_dir, file_name))\n",
    "        # print(hidden_representations.shape)\n",
    "        n_sequences = hidden_representations.shape[0]\n",
    "\n",
    "        if representations is None:\n",
    "            representations = hidden_representations\n",
    "        else:\n",
    "            representations = np.concatenate((representations, hidden_representations), axis=0)\n",
    "\n",
    "        classes += n_sequences * [idx] # assign representations to classes\n",
    "    \n",
    "    classes = np.asarray(classes)\n",
    "\n",
    "    # shuffle representations and classes\n",
    "    X, y = unison_shuffled_copies(representations, classes)\n",
    "    print(X.shape, y.shape)\n",
    "\n",
    "    # train linear classifier\n",
    "    # multi_class='multinomial' uses a CE loss\n",
    "    print('classifying between:', prompt_names)\n",
    "    clf = LogisticRegression(random_state=0, max_iter=2000, multi_class='multinomial').fit(X, y)\n",
    "    \n",
    "    print(f'layer={layer}; accuracy on training data: ', clf.score(X, y))\n",
    "    print('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
