{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from utils import load_hidden_representations_from_hdf5, read_templates_from_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "log_dir = \"/logfiles\"\n",
    "model = \"bigscience-T0\"\n",
    "module = \"decoder\"\n",
    "task = \"wic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assertions\n",
    "assert model == \"bigscience-T0\"\n",
    "assert module == \"decoder\"\n",
    "assert task == \"wic\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>template</th>\n",
       "      <th>category</th>\n",
       "      <th>includes_targets</th>\n",
       "      <th>targets</th>\n",
       "      <th>target_ids</th>\n",
       "      <th>shuffle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt_3_without_targets</td>\n",
       "      <td>{sentence1} {sentence2} Question: Is the word ...</td>\n",
       "      <td>instructive</td>\n",
       "      <td>False</td>\n",
       "      <td>▁No, ▁no, ▁Yes, ▁yes</td>\n",
       "      <td>0, 0, 1, 1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt_3_yes_no_with_targets</td>\n",
       "      <td>{sentence1} {sentence2} Question: Is the word ...</td>\n",
       "      <td>instructive</td>\n",
       "      <td>True</td>\n",
       "      <td>▁No, ▁Yes</td>\n",
       "      <td>0, 1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>affirmation_with_targets</td>\n",
       "      <td>Sentence A: {sentence1} Sentence B: {sentence2...</td>\n",
       "      <td>instructive</td>\n",
       "      <td>True</td>\n",
       "      <td>▁Fal, ▁True</td>\n",
       "      <td>0, 1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>grammar_homework_with_targets</td>\n",
       "      <td>Decide whether the word \"{word}\" is used with ...</td>\n",
       "      <td>instructive</td>\n",
       "      <td>True</td>\n",
       "      <td>▁No, ▁Yes</td>\n",
       "      <td>0, 1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>polysemous_with_targets</td>\n",
       "      <td>The word \"{word}\" has multiple meanings. Does ...</td>\n",
       "      <td>instructive</td>\n",
       "      <td>True</td>\n",
       "      <td>▁No, ▁Yes</td>\n",
       "      <td>0, 1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>question_context_without_targets</td>\n",
       "      <td>Determine if the word \"{word}\" is used in the ...</td>\n",
       "      <td>instructive</td>\n",
       "      <td>False</td>\n",
       "      <td>▁No, ▁no, ▁Yes, ▁yes</td>\n",
       "      <td>0, 0, 1, 1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>question_meaning_without_targets</td>\n",
       "      <td>Determine if the word \"{word}\" have the same m...</td>\n",
       "      <td>instructive</td>\n",
       "      <td>False</td>\n",
       "      <td>▁No, ▁no, ▁Yes, ▁yes</td>\n",
       "      <td>0, 0, 1, 1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>question_meaning_yes_no_with_targets</td>\n",
       "      <td>Does the word \"{word}\" have the same meanining...</td>\n",
       "      <td>instructive</td>\n",
       "      <td>True</td>\n",
       "      <td>▁No, ▁Yes</td>\n",
       "      <td>0, 1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>same_sense_with_targets</td>\n",
       "      <td>Sentence 1: {sentence1} Sentence 2: {sentence2...</td>\n",
       "      <td>instructive</td>\n",
       "      <td>True</td>\n",
       "      <td>▁No, ▁Yes</td>\n",
       "      <td>0, 1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>similar_sense_without_targets</td>\n",
       "      <td>{sentence1} {sentence2} Similar sense of {word}?</td>\n",
       "      <td>instructive</td>\n",
       "      <td>False</td>\n",
       "      <td>▁No, ▁no, ▁Yes, ▁yes</td>\n",
       "      <td>0, 0, 1, 1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>similar_sense_yes_no_with_targets</td>\n",
       "      <td>{sentence1} {sentence2} Similar sense of {word...</td>\n",
       "      <td>instructive</td>\n",
       "      <td>True</td>\n",
       "      <td>▁No, ▁Yes</td>\n",
       "      <td>0, 1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>gpt_3_true_false_with_targets</td>\n",
       "      <td>{sentence1} {sentence2} Question: Is the word ...</td>\n",
       "      <td>instructive</td>\n",
       "      <td>True</td>\n",
       "      <td>▁Fal, ▁True</td>\n",
       "      <td>0, 1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>gpt_3_cat_dog_with_targets</td>\n",
       "      <td>{sentence1} {sentence2} Question: Is the word ...</td>\n",
       "      <td>instructive</td>\n",
       "      <td>True</td>\n",
       "      <td>▁Cat, ▁Dog</td>\n",
       "      <td>0, 1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    name  \\\n",
       "0                  gpt_3_without_targets   \n",
       "1              gpt_3_yes_no_with_targets   \n",
       "2               affirmation_with_targets   \n",
       "3          grammar_homework_with_targets   \n",
       "4                polysemous_with_targets   \n",
       "5       question_context_without_targets   \n",
       "6       question_meaning_without_targets   \n",
       "7   question_meaning_yes_no_with_targets   \n",
       "8                same_sense_with_targets   \n",
       "9          similar_sense_without_targets   \n",
       "10     similar_sense_yes_no_with_targets   \n",
       "11         gpt_3_true_false_with_targets   \n",
       "12            gpt_3_cat_dog_with_targets   \n",
       "\n",
       "                                             template     category  \\\n",
       "0   {sentence1} {sentence2} Question: Is the word ...  instructive   \n",
       "1   {sentence1} {sentence2} Question: Is the word ...  instructive   \n",
       "2   Sentence A: {sentence1} Sentence B: {sentence2...  instructive   \n",
       "3   Decide whether the word \"{word}\" is used with ...  instructive   \n",
       "4   The word \"{word}\" has multiple meanings. Does ...  instructive   \n",
       "5   Determine if the word \"{word}\" is used in the ...  instructive   \n",
       "6   Determine if the word \"{word}\" have the same m...  instructive   \n",
       "7   Does the word \"{word}\" have the same meanining...  instructive   \n",
       "8   Sentence 1: {sentence1} Sentence 2: {sentence2...  instructive   \n",
       "9    {sentence1} {sentence2} Similar sense of {word}?  instructive   \n",
       "10  {sentence1} {sentence2} Similar sense of {word...  instructive   \n",
       "11  {sentence1} {sentence2} Question: Is the word ...  instructive   \n",
       "12  {sentence1} {sentence2} Question: Is the word ...  instructive   \n",
       "\n",
       "    includes_targets               targets  target_ids  shuffle  \n",
       "0              False  ▁No, ▁no, ▁Yes, ▁yes  0, 0, 1, 1    False  \n",
       "1               True             ▁No, ▁Yes        0, 1    False  \n",
       "2               True           ▁Fal, ▁True        0, 1    False  \n",
       "3               True             ▁No, ▁Yes        0, 1    False  \n",
       "4               True             ▁No, ▁Yes        0, 1    False  \n",
       "5              False  ▁No, ▁no, ▁Yes, ▁yes  0, 0, 1, 1    False  \n",
       "6              False  ▁No, ▁no, ▁Yes, ▁yes  0, 0, 1, 1    False  \n",
       "7               True             ▁No, ▁Yes        0, 1    False  \n",
       "8               True             ▁No, ▁Yes        0, 1    False  \n",
       "9              False  ▁No, ▁no, ▁Yes, ▁yes  0, 0, 1, 1    False  \n",
       "10              True             ▁No, ▁Yes        0, 1    False  \n",
       "11              True           ▁Fal, ▁True        0, 1    False  \n",
       "12              True            ▁Cat, ▁Dog        0, 1    False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = read_templates_from_file(f\"/t0-analysis/prompts/{task}/all.csv\")\n",
    "# df = read_templates_from_file(f\"/t0-analysis/prompts/{task}/fixed_prompt.csv\")\n",
    "# df = read_templates_from_file(f\"/t0-analysis/prompts/{task}/fixed_target_yes_no.csv\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train linear classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unison_shuffled_copies(a, b):\n",
    "    # from: https://stackoverflow.com/questions/4601373/better-way-to-shuffle-two-numpy-arrays-in-unison\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_pattern = [\n",
    "    # \"gpt_3_without_targets\",\n",
    "    \"gpt_3_yes_no_with_targets\",\n",
    "    # \"affirmation_with_targets\",\n",
    "    # \"grammar_homework_with_targets\",\n",
    "    \"polysemous_with_targets\",\n",
    "    # \"question_context_without_targets\",\n",
    "    # \"question_meaning_without_targets\",\n",
    "    # \"question_meaning_yes_no_with_targets\",\n",
    "    # \"same_sense_with_targets\",\n",
    "    # \"similar_sense_without_targets\",\n",
    "    # \"similar_sense_yes_no_with_targets\",\n",
    "    # \"gpt_3_true_false_with_targets\",\n",
    "    # \"gpt_3_cat_dog_with_targets\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token: 0\tlayer: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 5520.43it/s]\n",
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 4315.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1276, 4096) (1276,)\n",
      "classifying between: ['gpt_3_yes_no_with_targets', 'polysemous_with_targets']\n",
      "token=0; layer=0; accuracy on training data:  0.5\n",
      "\n",
      "\n",
      "token: 0\tlayer: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 4900.01it/s]\n",
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 3886.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1276, 4096) (1276,)\n",
      "classifying between: ['gpt_3_yes_no_with_targets', 'polysemous_with_targets']\n",
      "token=0; layer=1; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "token: 0\tlayer: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 4337.30it/s]\n",
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 4237.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1276, 4096) (1276,)\n",
      "classifying between: ['gpt_3_yes_no_with_targets', 'polysemous_with_targets']\n",
      "token=0; layer=2; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "token: 0\tlayer: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 4547.98it/s]\n",
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 4113.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1276, 4096) (1276,)\n",
      "classifying between: ['gpt_3_yes_no_with_targets', 'polysemous_with_targets']\n",
      "token=0; layer=3; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "token: 0\tlayer: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 4070.12it/s]\n",
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 3846.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1276, 4096) (1276,)\n",
      "classifying between: ['gpt_3_yes_no_with_targets', 'polysemous_with_targets']\n",
      "token=0; layer=4; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "token: 0\tlayer: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 4706.62it/s]\n",
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 2768.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1276, 4096) (1276,)\n",
      "classifying between: ['gpt_3_yes_no_with_targets', 'polysemous_with_targets']\n",
      "token=0; layer=5; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "token: 0\tlayer: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 3443.94it/s]\n",
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 4196.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1276, 4096) (1276,)\n",
      "classifying between: ['gpt_3_yes_no_with_targets', 'polysemous_with_targets']\n",
      "token=0; layer=6; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "token: 0\tlayer: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 4799.29it/s]\n",
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 3404.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1276, 4096) (1276,)\n",
      "classifying between: ['gpt_3_yes_no_with_targets', 'polysemous_with_targets']\n",
      "token=0; layer=7; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "token: 0\tlayer: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 4879.81it/s]\n",
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 4951.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1276, 4096) (1276,)\n",
      "classifying between: ['gpt_3_yes_no_with_targets', 'polysemous_with_targets']\n",
      "token=0; layer=8; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "token: 0\tlayer: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 4620.42it/s]\n",
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 3113.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1276, 4096) (1276,)\n",
      "classifying between: ['gpt_3_yes_no_with_targets', 'polysemous_with_targets']\n",
      "token=0; layer=9; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "token: 0\tlayer: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 4033.90it/s]\n",
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 4826.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1276, 4096) (1276,)\n",
      "classifying between: ['gpt_3_yes_no_with_targets', 'polysemous_with_targets']\n",
      "token=0; layer=10; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "token: 0\tlayer: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 4363.48it/s]\n",
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 4059.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1276, 4096) (1276,)\n",
      "classifying between: ['gpt_3_yes_no_with_targets', 'polysemous_with_targets']\n",
      "token=0; layer=11; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "token: 0\tlayer: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 4477.58it/s]\n",
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 5337.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1276, 4096) (1276,)\n",
      "classifying between: ['gpt_3_yes_no_with_targets', 'polysemous_with_targets']\n",
      "token=0; layer=12; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "token: 0\tlayer: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 4723.11it/s]\n",
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 4834.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1276, 4096) (1276,)\n",
      "classifying between: ['gpt_3_yes_no_with_targets', 'polysemous_with_targets']\n",
      "token=0; layer=13; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "token: 0\tlayer: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 4907.93it/s]\n",
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 4015.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1276, 4096) (1276,)\n",
      "classifying between: ['gpt_3_yes_no_with_targets', 'polysemous_with_targets']\n",
      "token=0; layer=14; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "token: 0\tlayer: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 4759.28it/s]\n",
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 3876.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1276, 4096) (1276,)\n",
      "classifying between: ['gpt_3_yes_no_with_targets', 'polysemous_with_targets']\n",
      "token=0; layer=15; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "token: 0\tlayer: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 3932.95it/s]\n",
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 4274.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1276, 4096) (1276,)\n",
      "classifying between: ['gpt_3_yes_no_with_targets', 'polysemous_with_targets']\n",
      "token=0; layer=16; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "token: 0\tlayer: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 4326.70it/s]\n",
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 3637.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1276, 4096) (1276,)\n",
      "classifying between: ['gpt_3_yes_no_with_targets', 'polysemous_with_targets']\n",
      "token=0; layer=17; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "token: 0\tlayer: 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 4758.97it/s]\n",
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 3142.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1276, 4096) (1276,)\n",
      "classifying between: ['gpt_3_yes_no_with_targets', 'polysemous_with_targets']\n",
      "token=0; layer=18; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "token: 0\tlayer: 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 4876.49it/s]\n",
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 4353.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1276, 4096) (1276,)\n",
      "classifying between: ['gpt_3_yes_no_with_targets', 'polysemous_with_targets']\n",
      "token=0; layer=19; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "token: 0\tlayer: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 5010.86it/s]\n",
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 3127.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1276, 4096) (1276,)\n",
      "classifying between: ['gpt_3_yes_no_with_targets', 'polysemous_with_targets']\n",
      "token=0; layer=20; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "token: 0\tlayer: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 4827.48it/s]\n",
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 3569.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1276, 4096) (1276,)\n",
      "classifying between: ['gpt_3_yes_no_with_targets', 'polysemous_with_targets']\n",
      "token=0; layer=21; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "token: 0\tlayer: 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 4855.54it/s]\n",
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 4115.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1276, 4096) (1276,)\n",
      "classifying between: ['gpt_3_yes_no_with_targets', 'polysemous_with_targets']\n",
      "token=0; layer=22; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "token: 0\tlayer: 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 5002.82it/s]\n",
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 3165.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1276, 4096) (1276,)\n",
      "classifying between: ['gpt_3_yes_no_with_targets', 'polysemous_with_targets']\n",
      "token=0; layer=23; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "token: 0\tlayer: 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 4925.19it/s]\n",
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 2567.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1276, 4096) (1276,)\n",
      "classifying between: ['gpt_3_yes_no_with_targets', 'polysemous_with_targets']\n",
      "token=0; layer=24; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "token: 1\tlayer: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 4756.12it/s]\n",
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 3975.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1276, 4096) (1276,)\n",
      "classifying between: ['gpt_3_yes_no_with_targets', 'polysemous_with_targets']\n",
      "token=1; layer=0; accuracy on training data:  0.5956112852664577\n",
      "\n",
      "\n",
      "token: 1\tlayer: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 4518.73it/s]\n",
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 4073.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1276, 4096) (1276,)\n",
      "classifying between: ['gpt_3_yes_no_with_targets', 'polysemous_with_targets']\n",
      "token=1; layer=1; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "token: 1\tlayer: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 4310.84it/s]\n",
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 3380.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1276, 4096) (1276,)\n",
      "classifying between: ['gpt_3_yes_no_with_targets', 'polysemous_with_targets']\n",
      "token=1; layer=2; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "token: 1\tlayer: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 4091.97it/s]\n",
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 3418.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1276, 4096) (1276,)\n",
      "classifying between: ['gpt_3_yes_no_with_targets', 'polysemous_with_targets']\n",
      "token=1; layer=3; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "token: 1\tlayer: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 4777.44it/s]\n",
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 4270.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1276, 4096) (1276,)\n",
      "classifying between: ['gpt_3_yes_no_with_targets', 'polysemous_with_targets']\n",
      "token=1; layer=4; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "token: 1\tlayer: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 3620.21it/s]\n",
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 3985.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1276, 4096) (1276,)\n",
      "classifying between: ['gpt_3_yes_no_with_targets', 'polysemous_with_targets']\n",
      "token=1; layer=5; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "token: 1\tlayer: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 3622.24it/s]\n",
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 4756.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1276, 4096) (1276,)\n",
      "classifying between: ['gpt_3_yes_no_with_targets', 'polysemous_with_targets']\n",
      "token=1; layer=6; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "token: 1\tlayer: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 4146.51it/s]\n",
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 3972.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1276, 4096) (1276,)\n",
      "classifying between: ['gpt_3_yes_no_with_targets', 'polysemous_with_targets']\n",
      "token=1; layer=7; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "token: 1\tlayer: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 4722.17it/s]\n",
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 2811.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1276, 4096) (1276,)\n",
      "classifying between: ['gpt_3_yes_no_with_targets', 'polysemous_with_targets']\n",
      "token=1; layer=8; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "token: 1\tlayer: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 4914.68it/s]\n",
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 3323.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1276, 4096) (1276,)\n",
      "classifying between: ['gpt_3_yes_no_with_targets', 'polysemous_with_targets']\n",
      "token=1; layer=9; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "token: 1\tlayer: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 4771.71it/s]\n",
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 2638.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1276, 4096) (1276,)\n",
      "classifying between: ['gpt_3_yes_no_with_targets', 'polysemous_with_targets']\n",
      "token=1; layer=10; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "token: 1\tlayer: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 4321.56it/s]\n",
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 3934.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1276, 4096) (1276,)\n",
      "classifying between: ['gpt_3_yes_no_with_targets', 'polysemous_with_targets']\n",
      "token=1; layer=11; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "token: 1\tlayer: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 4765.44it/s]\n",
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 3773.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1276, 4096) (1276,)\n",
      "classifying between: ['gpt_3_yes_no_with_targets', 'polysemous_with_targets']\n",
      "token=1; layer=12; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "token: 1\tlayer: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 4719.57it/s]\n",
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 3411.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1276, 4096) (1276,)\n",
      "classifying between: ['gpt_3_yes_no_with_targets', 'polysemous_with_targets']\n",
      "token=1; layer=13; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "token: 1\tlayer: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 4773.00it/s]\n",
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 2618.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1276, 4096) (1276,)\n",
      "classifying between: ['gpt_3_yes_no_with_targets', 'polysemous_with_targets']\n",
      "token=1; layer=14; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "token: 1\tlayer: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 4757.84it/s]\n",
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 3945.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1276, 4096) (1276,)\n",
      "classifying between: ['gpt_3_yes_no_with_targets', 'polysemous_with_targets']\n",
      "token=1; layer=15; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "token: 1\tlayer: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 4613.31it/s]\n",
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 3799.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1276, 4096) (1276,)\n",
      "classifying between: ['gpt_3_yes_no_with_targets', 'polysemous_with_targets']\n",
      "token=1; layer=16; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "token: 1\tlayer: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 4927.81it/s]\n",
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 3979.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1276, 4096) (1276,)\n",
      "classifying between: ['gpt_3_yes_no_with_targets', 'polysemous_with_targets']\n",
      "token=1; layer=17; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "token: 1\tlayer: 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 5036.44it/s]\n",
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 3021.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1276, 4096) (1276,)\n",
      "classifying between: ['gpt_3_yes_no_with_targets', 'polysemous_with_targets']\n",
      "token=1; layer=18; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "token: 1\tlayer: 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 4428.97it/s]\n",
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 3024.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1276, 4096) (1276,)\n",
      "classifying between: ['gpt_3_yes_no_with_targets', 'polysemous_with_targets']\n",
      "token=1; layer=19; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "token: 1\tlayer: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 4756.76it/s]\n",
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 4017.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1276, 4096) (1276,)\n",
      "classifying between: ['gpt_3_yes_no_with_targets', 'polysemous_with_targets']\n",
      "token=1; layer=20; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "token: 1\tlayer: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 5136.34it/s]\n",
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 2680.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1276, 4096) (1276,)\n",
      "classifying between: ['gpt_3_yes_no_with_targets', 'polysemous_with_targets']\n",
      "token=1; layer=21; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "token: 1\tlayer: 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 4481.12it/s]\n",
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 3111.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1276, 4096) (1276,)\n",
      "classifying between: ['gpt_3_yes_no_with_targets', 'polysemous_with_targets']\n",
      "token=1; layer=22; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "token: 1\tlayer: 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 4338.02it/s]\n",
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 4169.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1276, 4096) (1276,)\n",
      "classifying between: ['gpt_3_yes_no_with_targets', 'polysemous_with_targets']\n",
      "token=1; layer=23; accuracy on training data:  1.0\n",
      "\n",
      "\n",
      "token: 1\tlayer: 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 4505.99it/s]\n",
      "Reading embeddings: 100%|██████████| 638/638 [00:00<00:00, 3250.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1276, 4096) (1276,)\n",
      "classifying between: ['gpt_3_yes_no_with_targets', 'polysemous_with_targets']\n",
      "token=1; layer=24; accuracy on training data:  1.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in range(2):\n",
    "    # for layer in range(0, 1):\n",
    "    for layer in range(0, 25):\n",
    "    # for layer in range(24, 25):\n",
    "        \n",
    "        print(f\"token: {t}\\tlayer: {layer}\")\n",
    "        file_names, prompt_names = [], []\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            if row['name'] in use_pattern:\n",
    "                file_names.append(f\"{task}/{model}/{module}/{row['name']}/hidden_represenations_t{t}_layer{layer}_avg.hdf5\",)\n",
    "                prompt_names.append(row['name'])\n",
    "\n",
    "        # load hidden representations from hdf5 file\n",
    "        representations = None\n",
    "        classes = []\n",
    "        n_sequences = 0\n",
    "\n",
    "        for idx, file_name in enumerate(file_names):\n",
    "            hidden_representations = load_hidden_representations_from_hdf5(os.path.join(log_dir, file_name))\n",
    "            # print(hidden_representations.shape)\n",
    "            n_sequences = hidden_representations.shape[0]\n",
    "\n",
    "            if representations is None:\n",
    "                representations = hidden_representations\n",
    "            else:\n",
    "                representations = np.concatenate((representations, hidden_representations), axis=0)\n",
    "\n",
    "            classes += n_sequences * [idx] # assign representations to classes\n",
    "        \n",
    "        classes = np.asarray(classes)\n",
    "\n",
    "        # shuffle representations and classes\n",
    "        X, y = unison_shuffled_copies(representations, classes)\n",
    "        print(X.shape, y.shape)\n",
    "\n",
    "        # train linear classifier\n",
    "        # multi_class='multinomial' uses a CE loss\n",
    "        print('classifying between:', prompt_names)\n",
    "        clf = LogisticRegression(random_state=0, max_iter=2000, multi_class='multinomial').fit(X, y)\n",
    "        \n",
    "        print(f'token={t}; layer={layer}; accuracy on training data: ', clf.score(X, y))\n",
    "        print('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
